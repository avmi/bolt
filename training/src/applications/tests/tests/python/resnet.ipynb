{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "\n",
    "- Jira: http://jira-msc.rnd.huawei.com/browse/AEE-155\n",
    "- Base: https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pyraul.pipeline.train_step import train_step\n",
    "from pyraul.tools.dataset import Dataset\n",
    "from pyraul.tools.dumping import dump_weights, gen_cpp_dtVec, size\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BasicBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet18(**kwargs):\n",
    "    r\"\"\"ResNet-18 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "\n",
    "def resnet34(**kwargs):\n",
    "    r\"\"\"ResNet-34 moodel from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "def resnet50(**kwargs):\n",
    "    r\"\"\"ResNet-50 moodel from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "    \"\"\"\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "def resnet101(**kwargs):\n",
    "    r\"\"\"ResNet-101 moodel from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "    \"\"\"\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "\n",
    "def resnet152(**kwargs):\n",
    "    r\"\"\"ResNet-152 moodel from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "    \"\"\"\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet18(num_classes=10)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TestResNet, ResNet18CifarPretrainedModelOneEpochTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loading CIFAR10 dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "83.24\n",
      "Step    0/1000\tLoss: 0.036086 (0.036086)\tTime.step: 0.101 (0.101)\tTime.load: 0.026 (0.026)\n",
      "Step  100/1000\tLoss: 0.181785 (0.077186)\tTime.step: 0.099 (0.101)\tTime.load: 0.023 (0.025)\n",
      "Step  200/1000\tLoss: 0.103403 (0.073088)\tTime.step: 0.101 (0.101)\tTime.load: 0.024 (0.025)\n",
      "Step  300/1000\tLoss: 0.004846 (0.071017)\tTime.step: 0.101 (0.101)\tTime.load: 0.025 (0.025)\n",
      "Step  400/1000\tLoss: 0.034619 (0.069301)\tTime.step: 0.101 (0.101)\tTime.load: 0.024 (0.025)\n",
      "Step  500/1000\tLoss: 0.014463 (0.066271)\tTime.step: 0.101 (0.101)\tTime.load: 0.024 (0.025)\n",
      "Step  600/1000\tLoss: 0.020965 (0.061124)\tTime.step: 0.103 (0.101)\tTime.load: 0.024 (0.025)\n",
      "Step  700/1000\tLoss: 0.008074 (0.058939)\tTime.step: 0.102 (0.101)\tTime.load: 0.024 (0.025)\n",
      "Step  800/1000\tLoss: 0.025178 (0.054667)\tTime.step: 0.102 (0.102)\tTime.load: 0.025 (0.025)\n",
      "Step  900/1000\tLoss: 0.017559 (0.050749)\tTime.step: 0.102 (0.102)\tTime.load: 0.024 (0.025)\n",
      "85.24\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pyraul.tools.dumping import dump_weights, DumpMode\n",
    "from pyraul.tools.seed import set_seed\n",
    "import torchvision.transforms as transforms\n",
    "from pyraul.tools.dataset import Dataset\n",
    "from pyraul.pipeline import accuracy\n",
    "from pyraul.pipeline.train_step import train_step\n",
    "\n",
    "config = {\n",
    "    \"seed\": 0,\n",
    "    \"classes\": 10,\n",
    "    \"bias\": True,\n",
    "    \"batch_size\": 50,\n",
    "    \"device\": \"cuda\",\n",
    "#     \"device\": \"cpu\",\n",
    "    \"epochs\": 20,\n",
    "    \"sgd\": {\"lr\": 0.05}\n",
    "}\n",
    "\n",
    "set_seed(config[\"seed\"])\n",
    "\n",
    "device = torch.device(config[\"device\"])\n",
    "model = Classifier()\n",
    "model = model.to(device)\n",
    "\n",
    "state = torch.load(\"checkpoint_10\")\n",
    "model.load_state_dict(state[\"model_state_dict\"])\n",
    "\n",
    "# dump_weights(model, \"83.24.txt\", mode=DumpMode.flatten_transpose)\n",
    "# dump_weights(model, \"83.24.txt\", mode=DumpMode.transpose_flatten, filter=\"fc\")\n",
    "\n",
    "ds= Dataset(\"CIFAR10\",\n",
    "            train_transform=[transforms.Resize(224, interpolation=0), transforms.ToTensor()],\n",
    "            test_transform=[transforms.Resize(224, interpolation=0), transforms.ToTensor()],\n",
    "            **config)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=config[\"sgd\"][\"lr\"])\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.NLLLoss(reduction=\"mean\")\n",
    "\n",
    "accuracy_before = accuracy(\n",
    "        model=model,\n",
    "        dataloader=ds.test_loader,\n",
    "        **config,\n",
    ")\n",
    "\n",
    "print(accuracy_before)\n",
    "\n",
    "loss, _, _ = train_step(\n",
    "                    ds.train_loader, \n",
    "                    model,\n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    device,\n",
    "                    print_freq=100,\n",
    "                    verbose=True,\n",
    "                    loss_history=True\n",
    "                )\n",
    "\n",
    "accuracy_after = accuracy(\n",
    "    model=model,\n",
    "    dataloader=ds.test_loader,\n",
    "    **config,\n",
    ")\n",
    "print(accuracy_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "const raul::dtVec idealLosses{3.608587e-02_dt, 1.817846e-01_dt, 1.034031e-01_dt, 4.845553e-03_dt, 3.461881e-02_dt, 1.446302e-02_dt, 2.096494e-02_dt, 8.074160e-03_dt, 2.517764e-02_dt, 1.755940e-02_dt};\n"
     ]
    }
   ],
   "source": [
    "print(gen_cpp_dtVec(loss.history[::100], \"idealLosses\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TestResNet, BasicBlockBuilding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicBlock(\n",
      "  (conv1): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "5480\n"
     ]
    }
   ],
   "source": [
    "inplanes = 10\n",
    "planes = 20\n",
    "block = BasicBlock(inplanes, planes)\n",
    "print(block)\n",
    "print(size(block))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TestResNet, InputBlockBuilding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InputBlock(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      ")\n",
      "9536\n"
     ]
    }
   ],
   "source": [
    "class InputBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 =  nn.BatchNorm2d(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "    def __forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        return x\n",
    "    \n",
    "block = InputBlock()\n",
    "print(block)\n",
    "print(size(block))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TestResNet, ResNet18Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "11181642\n"
     ]
    }
   ],
   "source": [
    "block = resnet18(num_classes=10)\n",
    "print(block)\n",
    "print(size(block))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TestResNet, ResNet18Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "const raul::dtVec golden_tensor_out{1.403602e+00_dt, 9.141974e-03_dt, -2.788210e+00_dt, 2.950551e+00_dt, -1.179150e+00_dt, -2.007151e+00_dt, -3.162097e-01_dt, -8.549929e-01_dt, 1.268970e+00_dt, 3.833183e-01_dt};\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pyraul.tools.dumping import dump_weights, gen_cpp_dtVec, DumpMode \n",
    "from pyraul.tools.seed import set_seed\n",
    "\n",
    "config = {\n",
    "    \"seed\": 0,\n",
    "    \"classes\": 10\n",
    "}\n",
    "\n",
    "set_seed(config[\"seed\"])\n",
    "\n",
    "model = resnet18(num_classes=config[\"classes\"])\n",
    "model.eval()\n",
    "\n",
    "# dump_weights(model, \"init.txt\", mode=DumpMode.flatten_transpose)\n",
    "dump_weights(model, \"init.txt\", mode=DumpMode.transpose_flatten, filter=\"fc\")\n",
    "\n",
    "tensor_in = torch.from_numpy(np.ones((1, 3, 224, 224), dtype=np.float32)*5)\n",
    "tensor_out = model(tensor_in)\n",
    "print(gen_cpp_dtVec(tensor_out.data.flatten(), \"golden_tensor_out\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sizes of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight 9408\n",
      "layer1.0.conv1.weight 36864\n",
      "layer1.0.conv2.weight 36864\n",
      "layer1.1.conv1.weight 36864\n",
      "layer1.1.conv2.weight 36864\n",
      "layer2.0.conv1.weight 73728\n",
      "layer2.0.conv2.weight 147456\n",
      "layer2.1.conv1.weight 147456\n",
      "layer2.1.conv2.weight 147456\n",
      "layer3.0.conv1.weight 294912\n",
      "layer3.0.conv2.weight 589824\n",
      "layer3.1.conv1.weight 589824\n",
      "layer3.1.conv2.weight 589824\n",
      "layer4.0.conv1.weight 1179648\n",
      "layer4.0.conv2.weight 2359296\n",
      "layer4.1.conv1.weight 2359296\n",
      "layer4.1.conv2.weight 2359296\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "for (name, param) in model.named_parameters():\n",
    "    if param.requires_grad and \"conv\" in name:\n",
    "        print(name, reduce(lambda a,b: a*b, param.shape, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traininig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Up to 80% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loading CIFAR10 dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "10.0\n",
      "Epoch: 001/030\n",
      "Step    0/1000\tLoss: 2.415540 (2.415540)\tTime.step: 0.103 (0.103)\tTime.load: 0.025 (0.025)\n",
      "Step  100/1000\tLoss: 1.704273 (1.926329)\tTime.step: 0.102 (0.103)\tTime.load: 0.024 (0.025)\n",
      "Step  200/1000\tLoss: 1.557923 (1.790768)\tTime.step: 0.104 (0.103)\tTime.load: 0.025 (0.025)\n",
      "Step  300/1000\tLoss: 1.519650 (1.702788)\tTime.step: 0.104 (0.103)\tTime.load: 0.025 (0.025)\n",
      "Step  400/1000\tLoss: 1.504326 (1.630857)\tTime.step: 0.104 (0.103)\tTime.load: 0.025 (0.025)\n",
      "Step  500/1000\tLoss: 0.951241 (1.560308)\tTime.step: 0.104 (0.103)\tTime.load: 0.025 (0.025)\n",
      "Step  600/1000\tLoss: 1.104180 (1.498991)\tTime.step: 0.103 (0.104)\tTime.load: 0.024 (0.025)\n",
      "Step  700/1000\tLoss: 1.052298 (1.450008)\tTime.step: 0.104 (0.104)\tTime.load: 0.025 (0.025)\n",
      "Step  800/1000\tLoss: 0.908380 (1.403901)\tTime.step: 0.104 (0.104)\tTime.load: 0.024 (0.025)\n",
      "Step  900/1000\tLoss: 0.679503 (1.363849)\tTime.step: 0.104 (0.104)\tTime.load: 0.024 (0.025)\n",
      "63.68\n",
      "Epoch: 002/030\n",
      "Step    0/1000\tLoss: 0.972856 (0.972856)\tTime.step: 0.106 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  100/1000\tLoss: 1.161652 (0.946779)\tTime.step: 0.106 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  200/1000\tLoss: 0.738835 (0.937626)\tTime.step: 0.109 (0.106)\tTime.load: 0.028 (0.026)\n",
      "Step  300/1000\tLoss: 0.633486 (0.916049)\tTime.step: 0.106 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  400/1000\tLoss: 0.933945 (0.901402)\tTime.step: 0.105 (0.106)\tTime.load: 0.025 (0.026)\n",
      "Step  500/1000\tLoss: 0.515443 (0.885640)\tTime.step: 0.105 (0.106)\tTime.load: 0.025 (0.026)\n",
      "Step  600/1000\tLoss: 0.856766 (0.868470)\tTime.step: 0.105 (0.106)\tTime.load: 0.025 (0.026)\n",
      "Step  700/1000\tLoss: 0.773562 (0.854999)\tTime.step: 0.106 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  800/1000\tLoss: 0.639599 (0.839362)\tTime.step: 0.103 (0.106)\tTime.load: 0.024 (0.026)\n",
      "Step  900/1000\tLoss: 0.504842 (0.825501)\tTime.step: 0.105 (0.106)\tTime.load: 0.025 (0.026)\n",
      "72.49\n",
      "Epoch: 003/030\n",
      "Step    0/1000\tLoss: 0.622381 (0.622381)\tTime.step: 0.109 (0.109)\tTime.load: 0.031 (0.031)\n",
      "Step  100/1000\tLoss: 0.905721 (0.671789)\tTime.step: 0.107 (0.106)\tTime.load: 0.027 (0.026)\n",
      "Step  200/1000\tLoss: 0.443219 (0.669798)\tTime.step: 0.106 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  300/1000\tLoss: 0.496565 (0.661995)\tTime.step: 0.106 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  400/1000\tLoss: 0.680420 (0.658686)\tTime.step: 0.105 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  500/1000\tLoss: 0.380592 (0.649248)\tTime.step: 0.107 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  600/1000\tLoss: 0.608210 (0.637052)\tTime.step: 0.106 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  700/1000\tLoss: 0.654119 (0.629966)\tTime.step: 0.106 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  800/1000\tLoss: 0.470445 (0.619540)\tTime.step: 0.108 (0.106)\tTime.load: 0.028 (0.026)\n",
      "Step  900/1000\tLoss: 0.379267 (0.611330)\tTime.step: 0.105 (0.106)\tTime.load: 0.026 (0.026)\n",
      "77.97\n",
      "Epoch: 004/030\n",
      "Step    0/1000\tLoss: 0.514355 (0.514355)\tTime.step: 0.106 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  100/1000\tLoss: 0.718911 (0.520029)\tTime.step: 0.106 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  200/1000\tLoss: 0.435635 (0.515483)\tTime.step: 0.105 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  300/1000\tLoss: 0.359672 (0.512933)\tTime.step: 0.106 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  400/1000\tLoss: 0.542845 (0.511781)\tTime.step: 0.106 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  500/1000\tLoss: 0.279050 (0.505807)\tTime.step: 0.106 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  600/1000\tLoss: 0.535608 (0.493696)\tTime.step: 0.105 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  700/1000\tLoss: 0.578227 (0.488032)\tTime.step: 0.106 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  800/1000\tLoss: 0.276371 (0.478855)\tTime.step: 0.107 (0.106)\tTime.load: 0.027 (0.026)\n",
      "Step  900/1000\tLoss: 0.275085 (0.472378)\tTime.step: 0.106 (0.106)\tTime.load: 0.026 (0.026)\n",
      "78.05\n",
      "Epoch: 005/030\n",
      "Step    0/1000\tLoss: 0.296503 (0.296503)\tTime.step: 0.107 (0.107)\tTime.load: 0.027 (0.027)\n",
      "Step  100/1000\tLoss: 0.484617 (0.397524)\tTime.step: 0.106 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  200/1000\tLoss: 0.324917 (0.392658)\tTime.step: 0.107 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  300/1000\tLoss: 0.307327 (0.391438)\tTime.step: 0.106 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  400/1000\tLoss: 0.433208 (0.390106)\tTime.step: 0.107 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  500/1000\tLoss: 0.189318 (0.383988)\tTime.step: 0.106 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  600/1000\tLoss: 0.446684 (0.374505)\tTime.step: 0.104 (0.106)\tTime.load: 0.024 (0.026)\n",
      "Step  700/1000\tLoss: 0.486196 (0.369546)\tTime.step: 0.106 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  800/1000\tLoss: 0.236778 (0.361030)\tTime.step: 0.105 (0.106)\tTime.load: 0.025 (0.026)\n",
      "Step  900/1000\tLoss: 0.194887 (0.355038)\tTime.step: 0.105 (0.106)\tTime.load: 0.026 (0.026)\n",
      "78.98\n",
      "Epoch: 006/030\n",
      "Step    0/1000\tLoss: 0.261560 (0.261560)\tTime.step: 0.105 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  100/1000\tLoss: 0.226612 (0.269528)\tTime.step: 0.104 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  200/1000\tLoss: 0.242828 (0.277971)\tTime.step: 0.104 (0.104)\tTime.load: 0.025 (0.024)\n",
      "Step  300/1000\tLoss: 0.187883 (0.277033)\tTime.step: 0.104 (0.104)\tTime.load: 0.024 (0.024)\n",
      "Step  400/1000\tLoss: 0.227686 (0.276034)\tTime.step: 0.104 (0.104)\tTime.load: 0.024 (0.025)\n",
      "Step  500/1000\tLoss: 0.181778 (0.272594)\tTime.step: 0.105 (0.104)\tTime.load: 0.025 (0.024)\n",
      "Step  600/1000\tLoss: 0.433339 (0.264957)\tTime.step: 0.105 (0.104)\tTime.load: 0.025 (0.025)\n",
      "Step  700/1000\tLoss: 0.349943 (0.260993)\tTime.step: 0.104 (0.104)\tTime.load: 0.025 (0.025)\n",
      "Step  800/1000\tLoss: 0.222297 (0.256147)\tTime.step: 0.104 (0.104)\tTime.load: 0.024 (0.025)\n",
      "Step  900/1000\tLoss: 0.078027 (0.250942)\tTime.step: 0.105 (0.104)\tTime.load: 0.024 (0.025)\n",
      "80.95\n",
      "Epoch: 007/030\n",
      "Step    0/1000\tLoss: 0.188973 (0.188973)\tTime.step: 0.105 (0.105)\tTime.load: 0.026 (0.026)\n",
      "Step  100/1000\tLoss: 0.152020 (0.186067)\tTime.step: 0.104 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  200/1000\tLoss: 0.162426 (0.191267)\tTime.step: 0.105 (0.105)\tTime.load: 0.024 (0.025)\n",
      "Step  300/1000\tLoss: 0.262541 (0.194397)\tTime.step: 0.105 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  400/1000\tLoss: 0.115909 (0.197626)\tTime.step: 0.104 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  500/1000\tLoss: 0.128224 (0.196145)\tTime.step: 0.104 (0.105)\tTime.load: 0.024 (0.025)\n",
      "Step  600/1000\tLoss: 0.237538 (0.191062)\tTime.step: 0.104 (0.105)\tTime.load: 0.024 (0.025)\n",
      "Step  700/1000\tLoss: 0.162434 (0.187299)\tTime.step: 0.104 (0.105)\tTime.load: 0.024 (0.025)\n",
      "Step  800/1000\tLoss: 0.091112 (0.184077)\tTime.step: 0.104 (0.105)\tTime.load: 0.024 (0.025)\n",
      "Step  900/1000\tLoss: 0.102800 (0.179034)\tTime.step: 0.104 (0.105)\tTime.load: 0.024 (0.025)\n",
      "79.99\n",
      "Epoch: 008/030\n",
      "Step    0/1000\tLoss: 0.135692 (0.135692)\tTime.step: 0.105 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  100/1000\tLoss: 0.220187 (0.131707)\tTime.step: 0.104 (0.104)\tTime.load: 0.024 (0.024)\n",
      "Step  200/1000\tLoss: 0.148889 (0.135741)\tTime.step: 0.104 (0.104)\tTime.load: 0.025 (0.024)\n",
      "Step  300/1000\tLoss: 0.118416 (0.140871)\tTime.step: 0.104 (0.104)\tTime.load: 0.024 (0.024)\n",
      "Step  400/1000\tLoss: 0.066223 (0.140193)\tTime.step: 0.104 (0.104)\tTime.load: 0.025 (0.024)\n",
      "Step  500/1000\tLoss: 0.175267 (0.138850)\tTime.step: 0.105 (0.104)\tTime.load: 0.025 (0.024)\n",
      "Step  600/1000\tLoss: 0.189864 (0.136689)\tTime.step: 0.105 (0.104)\tTime.load: 0.025 (0.024)\n",
      "Step  700/1000\tLoss: 0.073816 (0.134318)\tTime.step: 0.105 (0.104)\tTime.load: 0.024 (0.025)\n",
      "Step  800/1000\tLoss: 0.091269 (0.131082)\tTime.step: 0.105 (0.104)\tTime.load: 0.025 (0.025)\n",
      "Step  900/1000\tLoss: 0.078114 (0.126288)\tTime.step: 0.105 (0.104)\tTime.load: 0.025 (0.025)\n",
      "77.48\n",
      "Epoch: 009/030\n",
      "Step    0/1000\tLoss: 0.026147 (0.026147)\tTime.step: 0.105 (0.105)\tTime.load: 0.026 (0.026)\n",
      "Step  100/1000\tLoss: 0.016132 (0.100421)\tTime.step: 0.105 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  200/1000\tLoss: 0.079146 (0.099894)\tTime.step: 0.104 (0.104)\tTime.load: 0.024 (0.025)\n",
      "Step  300/1000\tLoss: 0.088678 (0.101608)\tTime.step: 0.107 (0.104)\tTime.load: 0.027 (0.025)\n",
      "Step  400/1000\tLoss: 0.027038 (0.105707)\tTime.step: 0.104 (0.104)\tTime.load: 0.025 (0.025)\n",
      "Step  500/1000\tLoss: 0.076769 (0.103739)\tTime.step: 0.105 (0.104)\tTime.load: 0.025 (0.025)\n",
      "Step  600/1000\tLoss: 0.258627 (0.102279)\tTime.step: 0.104 (0.104)\tTime.load: 0.024 (0.025)\n",
      "Step  700/1000\tLoss: 0.096578 (0.101792)\tTime.step: 0.105 (0.104)\tTime.load: 0.024 (0.025)\n",
      "Step  800/1000\tLoss: 0.072599 (0.102392)\tTime.step: 0.105 (0.104)\tTime.load: 0.025 (0.025)\n",
      "Step  900/1000\tLoss: 0.107161 (0.100697)\tTime.step: 0.104 (0.104)\tTime.load: 0.025 (0.025)\n",
      "80.73\n",
      "Epoch: 010/030\n",
      "Step    0/1000\tLoss: 0.032410 (0.032410)\tTime.step: 0.106 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  100/1000\tLoss: 0.024039 (0.080283)\tTime.step: 0.105 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  200/1000\tLoss: 0.084556 (0.076612)\tTime.step: 0.104 (0.105)\tTime.load: 0.024 (0.025)\n",
      "Step  300/1000\tLoss: 0.082283 (0.076825)\tTime.step: 0.105 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  400/1000\tLoss: 0.050221 (0.079884)\tTime.step: 0.104 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  500/1000\tLoss: 0.090390 (0.082525)\tTime.step: 0.105 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  600/1000\tLoss: 0.059671 (0.080871)\tTime.step: 0.104 (0.105)\tTime.load: 0.024 (0.025)\n",
      "Step  700/1000\tLoss: 0.026112 (0.079531)\tTime.step: 0.105 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  800/1000\tLoss: 0.051078 (0.080591)\tTime.step: 0.104 (0.105)\tTime.load: 0.024 (0.025)\n",
      "Step  900/1000\tLoss: 0.010992 (0.079548)\tTime.step: 0.104 (0.105)\tTime.load: 0.025 (0.025)\n",
      "83.24\n",
      "Epoch: 011/030\n",
      "Step    0/1000\tLoss: 0.036086 (0.036086)\tTime.step: 0.105 (0.105)\tTime.load: 0.026 (0.026)\n",
      "Step  100/1000\tLoss: 0.227216 (0.058441)\tTime.step: 0.105 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  200/1000\tLoss: 0.007597 (0.066219)\tTime.step: 0.105 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  300/1000\tLoss: 0.039300 (0.064329)\tTime.step: 0.105 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  400/1000\tLoss: 0.054123 (0.067461)\tTime.step: 0.104 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  500/1000\tLoss: 0.019878 (0.067672)\tTime.step: 0.105 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  600/1000\tLoss: 0.011930 (0.065217)\tTime.step: 0.104 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  700/1000\tLoss: 0.044224 (0.063397)\tTime.step: 0.104 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  800/1000\tLoss: 0.005719 (0.062531)\tTime.step: 0.105 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  900/1000\tLoss: 0.056906 (0.062735)\tTime.step: 0.105 (0.105)\tTime.load: 0.025 (0.025)\n",
      "83.67\n",
      "Epoch: 012/030\n",
      "Step    0/1000\tLoss: 0.031814 (0.031814)\tTime.step: 0.105 (0.105)\tTime.load: 0.026 (0.026)\n",
      "Step  100/1000\tLoss: 0.089550 (0.051513)\tTime.step: 0.105 (0.105)\tTime.load: 0.024 (0.025)\n",
      "Step  200/1000\tLoss: 0.079906 (0.054194)\tTime.step: 0.105 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  300/1000\tLoss: 0.042887 (0.056376)\tTime.step: 0.105 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  400/1000\tLoss: 0.066443 (0.057637)\tTime.step: 0.105 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  500/1000\tLoss: 0.075283 (0.058838)\tTime.step: 0.104 (0.105)\tTime.load: 0.024 (0.025)\n",
      "Step  600/1000\tLoss: 0.060625 (0.060027)\tTime.step: 0.104 (0.105)\tTime.load: 0.024 (0.025)\n",
      "Step  700/1000\tLoss: 0.063136 (0.060272)\tTime.step: 0.104 (0.105)\tTime.load: 0.024 (0.025)\n",
      "Step  800/1000\tLoss: 0.072895 (0.060567)\tTime.step: 0.104 (0.105)\tTime.load: 0.024 (0.025)\n",
      "Step  900/1000\tLoss: 0.006298 (0.059252)\tTime.step: 0.104 (0.105)\tTime.load: 0.024 (0.025)\n",
      "83.29\n",
      "Epoch: 013/030\n",
      "Step    0/1000\tLoss: 0.030407 (0.030407)\tTime.step: 0.105 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  100/1000\tLoss: 0.024796 (0.048722)\tTime.step: 0.104 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  200/1000\tLoss: 0.043186 (0.048342)\tTime.step: 0.105 (0.105)\tTime.load: 0.024 (0.025)\n",
      "Step  300/1000\tLoss: 0.151250 (0.051856)\tTime.step: 0.104 (0.105)\tTime.load: 0.024 (0.025)\n",
      "Step  400/1000\tLoss: 0.080673 (0.051467)\tTime.step: 0.105 (0.105)\tTime.load: 0.024 (0.025)\n",
      "Step  500/1000\tLoss: 0.095795 (0.053351)\tTime.step: 0.104 (0.105)\tTime.load: 0.024 (0.025)\n",
      "Step  600/1000\tLoss: 0.149912 (0.054734)\tTime.step: 0.105 (0.105)\tTime.load: 0.024 (0.025)\n",
      "Step  700/1000\tLoss: 0.022708 (0.055348)\tTime.step: 0.105 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  800/1000\tLoss: 0.033347 (0.054652)\tTime.step: 0.104 (0.105)\tTime.load: 0.024 (0.025)\n",
      "Step  900/1000\tLoss: 0.144780 (0.053500)\tTime.step: 0.105 (0.105)\tTime.load: 0.024 (0.025)\n",
      "83.48\n",
      "Epoch: 014/030\n",
      "Step    0/1000\tLoss: 0.035395 (0.035395)\tTime.step: 0.106 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  100/1000\tLoss: 0.003136 (0.034997)\tTime.step: 0.104 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  200/1000\tLoss: 0.056047 (0.037341)\tTime.step: 0.104 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  300/1000\tLoss: 0.051398 (0.042614)\tTime.step: 0.104 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  400/1000\tLoss: 0.019688 (0.043277)\tTime.step: 0.105 (0.105)\tTime.load: 0.024 (0.025)\n",
      "Step  500/1000\tLoss: 0.001747 (0.043519)\tTime.step: 0.105 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  600/1000\tLoss: 0.029476 (0.042577)\tTime.step: 0.105 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  700/1000\tLoss: 0.031635 (0.042385)\tTime.step: 0.104 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  800/1000\tLoss: 0.005904 (0.043133)\tTime.step: 0.105 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  900/1000\tLoss: 0.083594 (0.044786)\tTime.step: 0.104 (0.105)\tTime.load: 0.024 (0.025)\n",
      "83.4\n",
      "Epoch: 015/030\n",
      "Step    0/1000\tLoss: 0.021831 (0.021831)\tTime.step: 0.105 (0.105)\tTime.load: 0.026 (0.026)\n",
      "Step  100/1000\tLoss: 0.010554 (0.046105)\tTime.step: 0.105 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  200/1000\tLoss: 0.005717 (0.051754)\tTime.step: 0.106 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  300/1000\tLoss: 0.010946 (0.049615)\tTime.step: 0.103 (0.105)\tTime.load: 0.023 (0.025)\n",
      "Step  400/1000\tLoss: 0.009135 (0.051199)\tTime.step: 0.104 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  500/1000\tLoss: 0.037259 (0.052215)\tTime.step: 0.104 (0.105)\tTime.load: 0.024 (0.025)\n",
      "Step  600/1000\tLoss: 0.012442 (0.049311)\tTime.step: 0.105 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  700/1000\tLoss: 0.172048 (0.047575)\tTime.step: 0.104 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  800/1000\tLoss: 0.008903 (0.046511)\tTime.step: 0.105 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  900/1000\tLoss: 0.040861 (0.045375)\tTime.step: 0.105 (0.105)\tTime.load: 0.025 (0.025)\n",
      "83.66\n",
      "Epoch: 016/030\n",
      "Step    0/1000\tLoss: 0.029962 (0.029962)\tTime.step: 0.104 (0.104)\tTime.load: 0.025 (0.025)\n",
      "Step  100/1000\tLoss: 0.005051 (0.034058)\tTime.step: 0.104 (0.104)\tTime.load: 0.024 (0.024)\n",
      "Step  200/1000\tLoss: 0.025087 (0.035785)\tTime.step: 0.103 (0.104)\tTime.load: 0.023 (0.025)\n",
      "Step  300/1000\tLoss: 0.139260 (0.038347)\tTime.step: 0.103 (0.104)\tTime.load: 0.023 (0.024)\n",
      "Step  400/1000\tLoss: 0.023443 (0.038682)\tTime.step: 0.102 (0.104)\tTime.load: 0.023 (0.024)\n",
      "Step  500/1000\tLoss: 0.005028 (0.038178)\tTime.step: 0.103 (0.104)\tTime.load: 0.023 (0.024)\n",
      "Step  600/1000\tLoss: 0.234796 (0.038013)\tTime.step: 0.107 (0.105)\tTime.load: 0.026 (0.025)\n",
      "Step  700/1000\tLoss: 0.007903 (0.040421)\tTime.step: 0.102 (0.105)\tTime.load: 0.022 (0.024)\n",
      "Step  800/1000\tLoss: 0.009158 (0.040297)\tTime.step: 0.102 (0.105)\tTime.load: 0.021 (0.024)\n",
      "Step  900/1000\tLoss: 0.044765 (0.040589)\tTime.step: 0.105 (0.104)\tTime.load: 0.025 (0.024)\n",
      "83.96\n",
      "Epoch: 017/030\n",
      "Step    0/1000\tLoss: 0.001913 (0.001913)\tTime.step: 0.106 (0.106)\tTime.load: 0.026 (0.026)\n",
      "Step  100/1000\tLoss: 0.005032 (0.030613)\tTime.step: 0.105 (0.105)\tTime.load: 0.025 (0.025)\n",
      "Step  200/1000\tLoss: 0.080171 (0.035999)\tTime.step: 0.106 (0.106)\tTime.load: 0.025 (0.026)\n"
     ]
    }
   ],
   "source": [
    "from pyraul.tools.seed import set_seed\n",
    "import torchvision.transforms as transforms\n",
    "from pyraul.tools.dataset import Dataset\n",
    "from pyraul.pipeline import accuracy\n",
    "from pyraul.pipeline.train_step import train_step\n",
    "from pyraul.tools.dumping import save_checkpoint\n",
    "\n",
    "config = {\n",
    "    \"seed\": 0,\n",
    "    \"classes\": 10,\n",
    "    \"bias\": True,\n",
    "    \"batch_size\": 50,\n",
    "    \"device\": \"cuda\",\n",
    "#     \"device\": \"cpu\",\n",
    "    \"epochs\": 30,\n",
    "    \"sgd\": {\"lr\": 0.05}\n",
    "}\n",
    "\n",
    "set_seed(config[\"seed\"])\n",
    "\n",
    "device = torch.device(config[\"device\"])\n",
    "model = Classifier()\n",
    "model = model.to(device)\n",
    "\n",
    "ds= Dataset(\"CIFAR10\",\n",
    "            train_transform=[transforms.Resize(224, interpolation=0), transforms.ToTensor()],\n",
    "            test_transform=[transforms.Resize(224, interpolation=0), transforms.ToTensor()],\n",
    "            **config)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=config[\"sgd\"][\"lr\"])\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.NLLLoss(reduction=\"mean\")\n",
    "\n",
    "accuracy_before = accuracy(\n",
    "        model=model,\n",
    "        dataloader=ds.test_loader,\n",
    "        **config,\n",
    ")\n",
    "\n",
    "print(accuracy_before)\n",
    "\n",
    "acc_history=[accuracy_before]\n",
    "loss_history=[]\n",
    "\n",
    "epochs = config[\"epochs\"]\n",
    "for epoch in range(1, epochs+1):\n",
    "        print(f\"Epoch: {epoch:03}/{epochs:03}\")\n",
    "        loss, _, _ = train_step(\n",
    "                            ds.train_loader, \n",
    "                            model,\n",
    "                            criterion,\n",
    "                            optimizer,\n",
    "                            device,\n",
    "                            print_freq=100,\n",
    "                            verbose=True,\n",
    "                            loss_history=True\n",
    "                        )\n",
    "\n",
    "        loss_history += loss.history\n",
    "        \n",
    "        accuracy_after = accuracy(\n",
    "            model=model,\n",
    "            dataloader=ds.test_loader,\n",
    "            **config,\n",
    "        )\n",
    "        print(accuracy_after)\n",
    "        acc_history.append(accuracy_after)\n",
    "        \n",
    "        save_checkpoint({\"epoch\": epoch,\n",
    "                         \"acc\": accuracy_after, \n",
    "                         \"acc_history\": acc_history,\n",
    "                         \"loss_history\": loss_history,\n",
    "                         \"model_state_dict\": model.state_dict(),\n",
    "                         \"optimizer_state_dict\": optimizer.state_dict()\n",
    "                        }, filename=f\"checkpoint_{epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input::bn1::Biases\", {\"bn1.bias\", 64}},\n",
      "{\"input::bn1::MeanEval\", {\"bn1.running_mean\", 64}},\n",
      "{\"input::bn1::VarianceEval\", {\"bn1.running_var\", 64}},\n",
      "{\"input::bn1::Weights\", {\"bn1.weight\", 64}},\n",
      "{\"layer1::block0::bn1::Biases\", {\"layer1.0.bn1.bias\", 64}},\n",
      "{\"layer1::block0::bn1::MeanEval\", {\"layer1.0.bn1.running_mean\", 64}},\n",
      "{\"layer1::block0::bn1::VarianceEval\", {\"layer1.0.bn1.running_var\", 64}},\n",
      "{\"layer1::block0::bn1::Weights\", {\"layer1.0.bn1.weight\", 64}},\n",
      "{\"layer1::block0::bn2::Biases\", {\"layer1.0.bn2.bias\", 64}},\n",
      "{\"layer1::block0::bn2::MeanEval\", {\"layer1.0.bn2.running_mean\", 64}},\n",
      "{\"layer1::block0::bn2::VarianceEval\", {\"layer1.0.bn2.running_var\", 64}},\n",
      "{\"layer1::block0::bn2::Weights\", {\"layer1.0.bn2.weight\", 64}},\n",
      "{\"layer1::block1::bn1::Biases\", {\"layer1.1.bn1.bias\", 64}},\n",
      "{\"layer1::block1::bn1::MeanEval\", {\"layer1.1.bn1.running_mean\", 64}},\n",
      "{\"layer1::block1::bn1::VarianceEval\", {\"layer1.1.bn1.running_var\", 64}},\n",
      "{\"layer1::block1::bn1::Weights\", {\"layer1.1.bn1.weight\", 64}},\n",
      "{\"layer1::block1::bn2::Biases\", {\"layer1.1.bn2.bias\", 64}},\n",
      "{\"layer1::block1::bn2::MeanEval\", {\"layer1.1.bn2.running_mean\", 64}},\n",
      "{\"layer1::block1::bn2::VarianceEval\", {\"layer1.1.bn2.running_var\", 64}},\n",
      "{\"layer1::block1::bn2::Weights\", {\"layer1.1.bn2.weight\", 64}},\n",
      "{\"layer2::block0::bn1::Biases\", {\"layer2.0.bn1.bias\", 128}},\n",
      "{\"layer2::block0::bn1::MeanEval\", {\"layer2.0.bn1.running_mean\", 128}},\n",
      "{\"layer2::block0::bn1::VarianceEval\", {\"layer2.0.bn1.running_var\", 128}},\n",
      "{\"layer2::block0::bn1::Weights\", {\"layer2.0.bn1.weight\", 128}},\n",
      "{\"layer2::block0::bn2::Biases\", {\"layer2.0.bn2.bias\", 128}},\n",
      "{\"layer2::block0::bn2::MeanEval\", {\"layer2.0.bn2.running_mean\", 128}},\n",
      "{\"layer2::block0::bn2::VarianceEval\", {\"layer2.0.bn2.running_var\", 128}},\n",
      "{\"layer2::block0::bn2::Weights\", {\"layer2.0.bn2.weight\", 128}},\n",
      "{\"layer2::block0::downsample::bn1::Biases\", {\"layer2.0.downsample.1.bias\", 128}},\n",
      "{\"layer2::block0::downsample::bn1::MeanEval\", {\"layer2.0.downsample.1.running_mean\", 128}},\n",
      "{\"layer2::block0::downsample::bn1::VarianceEval\", {\"layer2.0.downsample.1.running_var\", 128}},\n",
      "{\"layer2::block0::downsample::bn1::Weights\", {\"layer2.0.downsample.1.weight\", 128}},\n",
      "{\"layer2::block1::bn1::Biases\", {\"layer2.1.bn1.bias\", 128}},\n",
      "{\"layer2::block1::bn1::MeanEval\", {\"layer2.1.bn1.running_mean\", 128}},\n",
      "{\"layer2::block1::bn1::VarianceEval\", {\"layer2.1.bn1.running_var\", 128}},\n",
      "{\"layer2::block1::bn1::Weights\", {\"layer2.1.bn1.weight\", 128}},\n",
      "{\"layer2::block1::bn2::Biases\", {\"layer2.1.bn2.bias\", 128}},\n",
      "{\"layer2::block1::bn2::MeanEval\", {\"layer2.1.bn2.running_mean\", 128}},\n",
      "{\"layer2::block1::bn2::VarianceEval\", {\"layer2.1.bn2.running_var\", 128}},\n",
      "{\"layer2::block1::bn2::Weights\", {\"layer2.1.bn2.weight\", 128}},\n",
      "{\"layer3::block0::bn1::Biases\", {\"layer3.0.bn1.bias\", 256}},\n",
      "{\"layer3::block0::bn1::MeanEval\", {\"layer3.0.bn1.running_mean\", 256}},\n",
      "{\"layer3::block0::bn1::VarianceEval\", {\"layer3.0.bn1.running_var\", 256}},\n",
      "{\"layer3::block0::bn1::Weights\", {\"layer3.0.bn1.weight\", 256}},\n",
      "{\"layer3::block0::bn2::Biases\", {\"layer3.0.bn2.bias\", 256}},\n",
      "{\"layer3::block0::bn2::MeanEval\", {\"layer3.0.bn2.running_mean\", 256}},\n",
      "{\"layer3::block0::bn2::VarianceEval\", {\"layer3.0.bn2.running_var\", 256}},\n",
      "{\"layer3::block0::bn2::Weights\", {\"layer3.0.bn2.weight\", 256}},\n",
      "{\"layer3::block0::downsample::bn1::Biases\", {\"layer3.0.downsample.1.bias\", 256}},\n",
      "{\"layer3::block0::downsample::bn1::MeanEval\", {\"layer3.0.downsample.1.running_mean\", 256}},\n",
      "{\"layer3::block0::downsample::bn1::VarianceEval\", {\"layer3.0.downsample.1.running_var\", 256}},\n",
      "{\"layer3::block0::downsample::bn1::Weights\", {\"layer3.0.downsample.1.weight\", 256}},\n",
      "{\"layer3::block1::bn1::Biases\", {\"layer3.1.bn1.bias\", 256}},\n",
      "{\"layer3::block1::bn1::MeanEval\", {\"layer3.1.bn1.running_mean\", 256}},\n",
      "{\"layer3::block1::bn1::VarianceEval\", {\"layer3.1.bn1.running_var\", 256}},\n",
      "{\"layer3::block1::bn1::Weights\", {\"layer3.1.bn1.weight\", 256}},\n",
      "{\"layer3::block1::bn2::Biases\", {\"layer3.1.bn2.bias\", 256}},\n",
      "{\"layer3::block1::bn2::MeanEval\", {\"layer3.1.bn2.running_mean\", 256}},\n",
      "{\"layer3::block1::bn2::VarianceEval\", {\"layer3.1.bn2.running_var\", 256}},\n",
      "{\"layer3::block1::bn2::Weights\", {\"layer3.1.bn2.weight\", 256}},\n",
      "{\"layer4::block0::bn1::Biases\", {\"layer4.0.bn1.bias\", 512}},\n",
      "{\"layer4::block0::bn1::MeanEval\", {\"layer4.0.bn1.running_mean\", 512}},\n",
      "{\"layer4::block0::bn1::VarianceEval\", {\"layer4.0.bn1.running_var\", 512}},\n",
      "{\"layer4::block0::bn1::Weights\", {\"layer4.0.bn1.weight\", 512}},\n",
      "{\"layer4::block0::bn2::Biases\", {\"layer4.0.bn2.bias\", 512}},\n",
      "{\"layer4::block0::bn2::MeanEval\", {\"layer4.0.bn2.running_mean\", 512}},\n",
      "{\"layer4::block0::bn2::VarianceEval\", {\"layer4.0.bn2.running_var\", 512}},\n",
      "{\"layer4::block0::bn2::Weights\", {\"layer4.0.bn2.weight\", 512}},\n",
      "{\"layer4::block0::downsample::bn1::Biases\", {\"layer4.0.downsample.1.bias\", 512}},\n",
      "{\"layer4::block0::downsample::bn1::MeanEval\", {\"layer4.0.downsample.1.running_mean\", 512}},\n",
      "{\"layer4::block0::downsample::bn1::VarianceEval\", {\"layer4.0.downsample.1.running_var\", 512}},\n",
      "{\"layer4::block0::downsample::bn1::Weights\", {\"layer4.0.downsample.1.weight\", 512}},\n",
      "{\"layer4::block1::bn1::Biases\", {\"layer4.1.bn1.bias\", 512}},\n",
      "{\"layer4::block1::bn1::MeanEval\", {\"layer4.1.bn1.running_mean\", 512}},\n",
      "{\"layer4::block1::bn1::VarianceEval\", {\"layer4.1.bn1.running_var\", 512}},\n",
      "{\"layer4::block1::bn1::Weights\", {\"layer4.1.bn1.weight\", 512}},\n",
      "{\"layer4::block1::bn2::Biases\", {\"layer4.1.bn2.bias\", 512}},\n",
      "{\"layer4::block1::bn2::MeanEval\", {\"layer4.1.bn2.running_mean\", 512}},\n",
      "{\"layer4::block1::bn2::VarianceEval\", {\"layer4.1.bn2.running_var\", 512}},\n",
      "{\"layer4::block1::bn2::Weights\", {\"layer4.1.bn2.weight\", 512}},\n"
     ]
    }
   ],
   "source": [
    "a=[\n",
    "    \"input::bn1::Biases\",\n",
    "\"input::bn1::MeanEval\",\n",
    "\"input::bn1::VarianceEval\",\n",
    "\"input::bn1::Weights\",\n",
    "\"layer1::block0::bn1::Biases\",\n",
    "\"layer1::block0::bn1::MeanEval\",\n",
    "\"layer1::block0::bn1::VarianceEval\",\n",
    "\"layer1::block0::bn1::Weights\",\n",
    "\"layer1::block0::bn2::Biases\",\n",
    "\"layer1::block0::bn2::MeanEval\",\n",
    "\"layer1::block0::bn2::VarianceEval\",\n",
    "\"layer1::block0::bn2::Weights\",\n",
    "\"layer1::block1::bn1::Biases\",\n",
    "\"layer1::block1::bn1::MeanEval\",\n",
    "\"layer1::block1::bn1::VarianceEval\",\n",
    "\"layer1::block1::bn1::Weights\",\n",
    "\"layer1::block1::bn2::Biases\",\n",
    "\"layer1::block1::bn2::MeanEval\",\n",
    "\"layer1::block1::bn2::VarianceEval\",\n",
    "\"layer1::block1::bn2::Weights\",\n",
    "\"layer2::block0::bn1::Biases\",\n",
    "\"layer2::block0::bn1::MeanEval\",\n",
    "\"layer2::block0::bn1::VarianceEval\",\n",
    "\"layer2::block0::bn1::Weights\",\n",
    "\"layer2::block0::bn2::Biases\",\n",
    "\"layer2::block0::bn2::MeanEval\",\n",
    "\"layer2::block0::bn2::VarianceEval\",\n",
    "\"layer2::block0::bn2::Weights\",\n",
    "\"layer2::block0::downsample::bn1::Biases\",\n",
    "\"layer2::block0::downsample::bn1::MeanEval\",\n",
    "\"layer2::block0::downsample::bn1::VarianceEval\",\n",
    "\"layer2::block0::downsample::bn1::Weights\",\n",
    "\"layer2::block1::bn1::Biases\",\n",
    "\"layer2::block1::bn1::MeanEval\",\n",
    "\"layer2::block1::bn1::VarianceEval\",\n",
    "\"layer2::block1::bn1::Weights\",\n",
    "\"layer2::block1::bn2::Biases\",\n",
    "\"layer2::block1::bn2::MeanEval\",\n",
    "\"layer2::block1::bn2::VarianceEval\",\n",
    "\"layer2::block1::bn2::Weights\",\n",
    "\"layer3::block0::bn1::Biases\",\n",
    "\"layer3::block0::bn1::MeanEval\",\n",
    "\"layer3::block0::bn1::VarianceEval\",\n",
    "\"layer3::block0::bn1::Weights\",\n",
    "\"layer3::block0::bn2::Biases\",\n",
    "\"layer3::block0::bn2::MeanEval\",\n",
    "\"layer3::block0::bn2::VarianceEval\",\n",
    "\"layer3::block0::bn2::Weights\",\n",
    "\"layer3::block0::downsample::bn1::Biases\",\n",
    "\"layer3::block0::downsample::bn1::MeanEval\",\n",
    "\"layer3::block0::downsample::bn1::VarianceEval\",\n",
    "\"layer3::block0::downsample::bn1::Weights\",\n",
    "\"layer3::block1::bn1::Biases\",\n",
    "\"layer3::block1::bn1::MeanEval\",\n",
    "\"layer3::block1::bn1::VarianceEval\",\n",
    "\"layer3::block1::bn1::Weights\",\n",
    "\"layer3::block1::bn2::Biases\",\n",
    "\"layer3::block1::bn2::MeanEval\",\n",
    "\"layer3::block1::bn2::VarianceEval\",\n",
    "\"layer3::block1::bn2::Weights\",\n",
    "\"layer4::block0::bn1::Biases\",\n",
    "\"layer4::block0::bn1::MeanEval\",\n",
    "\"layer4::block0::bn1::VarianceEval\",\n",
    "\"layer4::block0::bn1::Weights\",\n",
    "\"layer4::block0::bn2::Biases\",\n",
    "\"layer4::block0::bn2::MeanEval\",\n",
    "\"layer4::block0::bn2::VarianceEval\",\n",
    "\"layer4::block0::bn2::Weights\",\n",
    "\"layer4::block0::downsample::bn1::Biases\",\n",
    "\"layer4::block0::downsample::bn1::MeanEval\",\n",
    "\"layer4::block0::downsample::bn1::VarianceEval\",\n",
    "\"layer4::block0::downsample::bn1::Weights\",\n",
    "\"layer4::block1::bn1::Biases\",\n",
    "\"layer4::block1::bn1::MeanEval\",\n",
    "\"layer4::block1::bn1::VarianceEval\",\n",
    "\"layer4::block1::bn1::Weights\",\n",
    "\"layer4::block1::bn2::Biases\",\n",
    "\"layer4::block1::bn2::MeanEval\",\n",
    "\"layer4::block1::bn2::VarianceEval\",\n",
    "\"layer4::block1::bn2::Weights\"\n",
    "]\n",
    "\n",
    "b = [\n",
    "  \"bn1.bias\",\n",
    "\"bn1.running_mean\",\n",
    "\"bn1.running_var\",\n",
    "\"bn1.weight\",\n",
    "\"layer1.0.bn1.bias\",\n",
    "\"layer1.0.bn1.running_mean\",\n",
    "\"layer1.0.bn1.running_var\",\n",
    "\"layer1.0.bn1.weight\",\n",
    "\"layer1.0.bn2.bias\",\n",
    "\"layer1.0.bn2.running_mean\",\n",
    "\"layer1.0.bn2.running_var\",\n",
    "\"layer1.0.bn2.weight\",\n",
    "\"layer1.1.bn1.bias\",\n",
    "\"layer1.1.bn1.running_mean\",\n",
    "\"layer1.1.bn1.running_var\",\n",
    "\"layer1.1.bn1.weight\",\n",
    "\"layer1.1.bn2.bias\",\n",
    "\"layer1.1.bn2.running_mean\",\n",
    "\"layer1.1.bn2.running_var\",\n",
    "\"layer1.1.bn2.weight\",\n",
    "\"layer2.0.bn1.bias\",\n",
    "\"layer2.0.bn1.running_mean\",\n",
    "\"layer2.0.bn1.running_var\",\n",
    "\"layer2.0.bn1.weight\",\n",
    "\"layer2.0.bn2.bias\",\n",
    "\"layer2.0.bn2.running_mean\",\n",
    "\"layer2.0.bn2.running_var\",\n",
    "\"layer2.0.bn2.weight\",\n",
    "\"layer2.0.downsample.1.bias\",\n",
    "\"layer2.0.downsample.1.running_mean\",\n",
    "\"layer2.0.downsample.1.running_var\",\n",
    "\"layer2.0.downsample.1.weight\",\n",
    "\"layer2.1.bn1.bias\",\n",
    "\"layer2.1.bn1.running_mean\",\n",
    "\"layer2.1.bn1.running_var\",\n",
    "\"layer2.1.bn1.weight\",\n",
    "\"layer2.1.bn2.bias\",\n",
    "\"layer2.1.bn2.running_mean\",\n",
    "\"layer2.1.bn2.running_var\",\n",
    "\"layer2.1.bn2.weight\",\n",
    "\"layer3.0.bn1.bias\",\n",
    "\"layer3.0.bn1.running_mean\",\n",
    "\"layer3.0.bn1.running_var\",\n",
    "\"layer3.0.bn1.weight\",\n",
    "\"layer3.0.bn2.bias\",\n",
    "\"layer3.0.bn2.running_mean\",\n",
    "\"layer3.0.bn2.running_var\",\n",
    "\"layer3.0.bn2.weight\",\n",
    "\"layer3.0.downsample.1.bias\",\n",
    "\"layer3.0.downsample.1.running_mean\",\n",
    "\"layer3.0.downsample.1.running_var\",\n",
    "\"layer3.0.downsample.1.weight\",\n",
    "\"layer3.1.bn1.bias\",\n",
    "\"layer3.1.bn1.running_mean\",\n",
    "\"layer3.1.bn1.running_var\",\n",
    "\"layer3.1.bn1.weight\",\n",
    "\"layer3.1.bn2.bias\",\n",
    "\"layer3.1.bn2.running_mean\",\n",
    "\"layer3.1.bn2.running_var\",\n",
    "\"layer3.1.bn2.weight\",\n",
    "\"layer4.0.bn1.bias\",\n",
    "\"layer4.0.bn1.running_mean\",\n",
    "\"layer4.0.bn1.running_var\",\n",
    "\"layer4.0.bn1.weight\",\n",
    "\"layer4.0.bn2.bias\",\n",
    "\"layer4.0.bn2.running_mean\",\n",
    "\"layer4.0.bn2.running_var\",\n",
    "\"layer4.0.bn2.weight\",\n",
    "\"layer4.0.downsample.1.bias\",\n",
    "\"layer4.0.downsample.1.running_mean\",\n",
    "\"layer4.0.downsample.1.running_var\",\n",
    "\"layer4.0.downsample.1.weight\",\n",
    "\"layer4.1.bn1.bias\",\n",
    "\"layer4.1.bn1.running_mean\",\n",
    "\"layer4.1.bn1.running_var\",\n",
    "\"layer4.1.bn1.weight\",\n",
    "\"layer4.1.bn2.bias\",\n",
    "\"layer4.1.bn2.running_mean\",\n",
    "\"layer4.1.bn2.running_var\",\n",
    "\"layer4.1.bn2.weight\"  \n",
    "]\n",
    "\n",
    "from functools import reduce\n",
    "data = {name: reduce(lambda x,y: x*y, param.shape, 1) for (name, param) in model.state_dict().items()}\n",
    "\n",
    "for x, y in zip(a,b):\n",
    "    print(f\"{{\\\"{x}\\\", {{\\\"{y}\\\", {data['resnet.'+y]}}}}},\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 1.4.0\n",
      "last updated: 2020-04-22 \n",
      "\n",
      "CPython 3.8.0\n",
      "IPython 7.13.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -d -u -v -iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
